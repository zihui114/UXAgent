"""
UX Analyzer -  LLM æ·±åº¦åˆ†æç‰ˆæœ¬

ä½¿ç”¨æ–¹å¼:
    python ux_analyzer_llm.py --run-dir runs/2026-01-19_22-29-07_c9ca --persona persona_old.txt

API Key æœƒè‡ªå‹•å¾ .env æª”æ¡ˆè®€å– OPENAI_API_KEY
"""

import json
import re
import os
import argparse
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from dotenv import load_dotenv
import openai


# ==================== Persona Profile ====================

class PersonaProfile:
    """è§£æä¸¦å„²å­˜ persona ç‰¹å¾µ"""

    def __init__(self, persona_file: str):
        self.raw_text = Path(persona_file).read_text(encoding='utf-8')
        self.name = self._extract_name()
        self.age = self._extract_field('Age')
        self.persona_type = self._extract_persona_type()
        self.risk_perception = self._extract_level('é¢¨éšªæ„ŸçŸ¥|Risk Perception')
        self.self_efficacy = self._extract_level('è‡ªæˆ‘æ•ˆèƒ½|Self-Efficacy')
        self.working_memory = self._extract_level('å·¥ä½œè¨˜æ†¶å®¹å¿åº¦|Working Memory Tolerance|Working Memory')
        self.strategy = self._extract_strategy()

    def _extract_name(self) -> str:
        # Try to find name like "Mrs. Wang" or "Mr. Chen"
        match = re.search(r'(Mrs?|Ms)\.\s+([A-Z][a-z]+)', self.raw_text)
        if match:
            return f"{match.group(1)}. {match.group(2)}"

        # Try Chinese name format
        match = re.search(r'(ç‹|é™³|æ|å¼µ|æ—|å³|é»ƒ|å‘¨|å¾|å­«|é¦¬|æœ±|èƒ¡|éƒ­|ä½•|é«˜|ç¾…|é„­|æ¢|è¬|å®‹|å”|è¨±|é„§|é¦®|éŸ“|æ›¹|æ›¾|å½­|è•­|è”¡|æ½˜|ç”°|è‘£|è¢|æ–¼|ä½™|è‘‰|è”£|æœ|è˜‡|é­|ç¨‹|å‘‚|ä¸|æ²ˆ|ä»»|å§š|ç›§|å‚…|é¾|å§œ|å´”|è­š|å»–|èŒƒ|æ±ª|é™¸|é‡‘|çŸ³|æˆ´|è³ˆ|éŸ‹|å¤|é‚±|æ–¹|ä¾¯|é„’|ç†Š|å­Ÿ|ç§¦|ç™½|æ±Ÿ|é–»|è–›|å°¹|æ®µ|é›·|é»|å²|é¾|é™¶|è³€|é¡§|æ¯›|éƒ|é¾”|é‚µ|è¬|éŒ¢|åš´|è¦ƒ|æ­¦|æˆ´|è«|å­”|å‘|å¸¸)(å…ˆç”Ÿ|å¥³å£«|å¤ªå¤ª)', self.raw_text)
        if match:
            return f"{match.group(1)}{match.group(2)}"
        
        # Try to find any name after "Name:" or "å§“å:"
        match = re.search(r'(?:Name|å§“å)[ï¼š:]\s*([^\n]+)', self.raw_text, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        return "Unknown"

    def _extract_field(self, pattern: str) -> str:
        match = re.search(f'({pattern})[ï¼š:]\\s*([^\n]+)', self.raw_text, re.IGNORECASE)
        return match.group(2).strip() if match else "Unknown"

    def _extract_persona_type(self) -> str:
        text_upper = self.raw_text.upper()
        if 'è¬¹æ…é©—è­‰å‹' in self.raw_text or 'CAUTIOUS VERIFIER' in text_upper or 'CAUTIOUS' in text_upper:
            return 'cautious_verifier'
        elif 'æ•ˆç‡å°å‘å‹' in self.raw_text or 'ROUTINE BUYER' in text_upper or 'EFFICIENCY' in text_upper:
            return 'efficiency_oriented'
        elif 'OLD' in text_upper or 'å¹´é•·' in self.raw_text or 'è€å¹´' in self.raw_text:
            return 'elderly_user'
        return 'unknown'

    def _extract_level(self, pattern: str) -> str:
        match = re.search(f'({pattern})[ï¼š:]\\s*([^\n]+)', self.raw_text, re.IGNORECASE)
        if match:
            level_text = match.group(2).upper()
            if 'HIGH' in level_text or 'é«˜' in level_text:
                return 'HIGH'
            elif 'LOW' in level_text or 'ä½' in level_text:
                return 'LOW'
            elif 'MEDIUM' in level_text or 'ä¸­' in level_text:
                return 'MEDIUM'
        return 'UNKNOWN'

    def _extract_strategy(self) -> str:
        if 'VERIFICATION-ORIENTED' in self.raw_text or 'é©—è­‰å°å‘' in self.raw_text:
            return 'verification'
        elif 'GOAL-ORIENTED' in self.raw_text or 'ç›®æ¨™å°å‘' in self.raw_text:
            return 'goal_oriented'
        return 'unknown'

    def to_dict(self) -> Dict[str, str]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            'name': self.name,
            'type': self.persona_type,
            'risk_perception': self.risk_perception,
            'self_efficacy': self.self_efficacy,
            'working_memory': self.working_memory,
            'strategy': self.strategy
        }


# ==================== LLM Analyzer ====================

class LLMUXAnalyzer:
    """ä½¿ç”¨ LLM é€²è¡Œæ™ºèƒ½ UX åˆ†æ"""

    def __init__(self, api_key: str = None):
        load_dotenv()
        self.api_key = api_key or os.environ.get('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError(
                "è«‹æä¾› OpenAI API key:\n"
                "1. åœ¨ .env æª”æ¡ˆä¸­è¨­å®š: OPENAI_API_KEY=your_key\n"
                "2. æˆ–ä½¿ç”¨ --api-key åƒæ•¸"
            )
        self.client = openai.OpenAI(api_key=self.api_key)

    def analyze_test_results(
        self,
        memory_trace: List[Dict],
        action_trace: List[str],
        persona: PersonaProfile,
        persona_file: str = None,
        run_dir: str = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM åˆ†ææ¸¬è©¦çµæœ"""
        
        # æº–å‚™æ•¸æ“šçµ¦ LLM
        analysis_data = self._prepare_analysis_data(
            memory_trace, action_trace, persona
        )

        # å‘¼å« LLM
        print("ğŸ¤– æ­£åœ¨ä½¿ç”¨ GPT-4 åˆ†ææ¸¬è©¦çµæœ...")

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ UX ç ”ç©¶å°ˆå®¶ã€‚è«‹å°ä»¥ä¸‹æ¸¬è©¦æ•¸æ“šé€²è¡Œæ·±åº¦åˆ†æï¼Œæ‰¾å‡ºæ‰€æœ‰ UX å•é¡Œä¸¦æ ¹æ“šå¯¦éš›çš„ HTML æ¨™ç±¤æä¾›å¯åŸ·è¡Œçš„æ”¹å–„å»ºè­°ã€‚"
                    },
                    {
                        "role": "user",
                        "content": self._build_analysis_prompt(analysis_data)
                    }
                ],
                temperature=0.1,
                max_completion_tokens=12000,
                response_format={"type": "json_object"}
            )

            # è§£æ LLM å›æ‡‰
            analysis_result = self._parse_llm_response(response.choices[0].message.content)

            # æ·»åŠ æ¸¬è©¦å…ƒæ•¸æ“š
            test_metadata = {
                "persona_file": persona_file or "Unknown",
                "persona_name": persona.name,
                "persona_type": persona.persona_type,
                "risk_perception": persona.risk_perception,
                "self_efficacy": persona.self_efficacy,
                "working_memory": persona.working_memory,
                "strategy": persona.strategy,
                "run_directory": run_dir or "Unknown",
                "analysis_timestamp": datetime.now().isoformat(),
                "total_actions": analysis_data.get('total_actions', 0),
                "total_thoughts": analysis_data.get('total_thoughts', 0)
            }

            result_with_metadata = {"test_metadata": test_metadata}
            if 'test_metadata' in analysis_result:
                del analysis_result['test_metadata']
            result_with_metadata.update(analysis_result)

        except openai.RateLimitError:
            print("âŒ API é…é¡å·²ç”¨ç›¡æˆ–é”åˆ°é€Ÿç‡é™åˆ¶")
            raise
        except openai.APIError as e:
            print(f"âŒ OpenAI API éŒ¯èª¤: {e}")
            raise

        return result_with_metadata

    def _prepare_analysis_data(
        self,
        memory_trace: List[Dict],
        action_trace: List[str],
        persona: PersonaProfile
    ) -> Dict[str, Any]:
        """æº–å‚™çµ¦ LLM çš„åˆ†ææ•¸æ“š"""

        # æå–ä¸­æ–‡æ€è€ƒ
        chinese_thoughts = []
        for entry in memory_trace:
            if entry.get('kind') == 'thought':
                content = entry.get('content', '')
                if any('\u4e00' <= char <= '\u9fff' for char in content):
                    chinese_thoughts.append({
                        'timestamp': entry.get('timestamp', 0),
                        'content': content
                    })

        # è§£æå‹•ä½œåºåˆ—
        actions = []
        element_info_map = {}
        for action_str in action_trace:
            try:
                action = json.loads(action_str)
                if isinstance(action, dict) and "action" in action:
                    actual_action = action["action"]
                    element_info = action.get("element_info", {})
                    target_id = actual_action.get("target", "")
                    if target_id and element_info:
                        element_info_map[target_id] = element_info
                    actions.append(actual_action)
                else:
                    actions.append(action)
            except:
                pass

        # ç²¾ç¢ºçµ±è¨ˆå‹•ä½œæ¬¡æ•¸
        target_click_count = {}
        for action in actions:
            target = action.get('target', '')
            if target:
                target_click_count[target] = target_click_count.get(target, 0) + 1

        # å»ºç«‹å‹•ä½œåºåˆ—æ‘˜è¦
        action_summary = []
        for i, action in enumerate(actions):
            action_summary.append({
                'step': i + 1,
                'target': action.get('target', ''),
                'description': action.get('description', ''),
                'action_type': action.get('action', 'click')
            })

        # æ‰¾å‡ºé‡è¤‡é»æ“Šçš„å…ƒç´ 
        repeated_clicks = {k: v for k, v in target_click_count.items() if v >= 2}

        # é™¤éŒ¯è¼¸å‡º
        print(f"\nğŸ“ æå–åˆ° {len(element_info_map)} å€‹å…ƒç´ çš„è³‡è¨Šæ˜ å°„")
        if element_info_map:
            print("ç¯„ä¾‹å…ƒç´ è³‡è¨Š:")
            for target_id, info in list(element_info_map.items())[:3]:
                print(f"  - {target_id}: class='{info.get('class', '')}', id='{info.get('id', '')}', tag='{info.get('tag', '')}'")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæœªæå–åˆ°ä»»ä½•å…ƒç´ è³‡è¨Šï¼")

        return {
            'persona_info': persona.to_dict(),
            'persona_text_excerpt': persona.raw_text[:1500],
            'total_actions': len(actions),
            'actions': actions,
            'element_info_map': element_info_map,
            'chinese_thoughts': chinese_thoughts,
            'total_thoughts': len(chinese_thoughts),
            'target_click_count': target_click_count,
            'action_summary': action_summary,
            'repeated_clicks': repeated_clicks,
        }

    def _build_analysis_prompt(self, data: Dict[str, Any]) -> str:
        """æ§‹å»ºçµ¦ LLM çš„åˆ†æ prompt"""

        prompt = f"""ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ UX ç ”ç©¶å°ˆå®¶ã€‚è«‹å°ä»¥ä¸‹æ¸¬è©¦æ•¸æ“šé€²è¡Œæ·±åº¦åˆ†æï¼Œæ‰¾å‡ºæ‰€æœ‰ UX å•é¡Œä¸¦æ ¹æ“šå¯¦éš›çš„ HTML æ¨™ç±¤æä¾›å¯åŸ·è¡Œçš„æ”¹å–„å»ºè­°ã€‚

# Persona è³‡è¨Š
- **å§“å**: {data['persona_info']['name']}
- **é¡å‹**: {data['persona_info']['type']}
- **é¢¨éšªæ„ŸçŸ¥**: {data['persona_info']['risk_perception']}
- **è‡ªæˆ‘æ•ˆèƒ½**: {data['persona_info']['self_efficacy']}
- **å·¥ä½œè¨˜æ†¶**: {data['persona_info']['working_memory']}
- **ä»»å‹™ç­–ç•¥**: {data['persona_info']['strategy']}

## Persona ç‰¹å¾µæè¿°
{data['persona_text_excerpt']}

---

# æ¸¬è©¦æ•¸æ“š

## ğŸ”¥ ç²¾ç¢ºçµ±è¨ˆæ•¸æ“šï¼ˆåªèƒ½å¼•ç”¨é€™äº›æ•¸å­—ï¼Œç¦æ­¢ç·¨é€ ï¼‰

### ç¸½å‹•ä½œæ¬¡æ•¸
**{data['total_actions']} å€‹å‹•ä½œ**

### æ¯å€‹ target çš„é»æ“Šæ¬¡æ•¸
```json
{json.dumps(data.get('target_click_count', {}), ensure_ascii=False, indent=2)}
```

### é‡è¤‡é»æ“Šçš„å…ƒç´ ï¼ˆé»æ“Š >= 2 æ¬¡ï¼‰
```json
{json.dumps(data.get('repeated_clicks', {}), ensure_ascii=False, indent=2)}
```

### å‹•ä½œåºåˆ—æ‘˜è¦
```json
{json.dumps(data.get('action_summary', [])[:30], ensure_ascii=False, indent=2)}
```

---

## ğŸ“ å…ƒç´ è³‡è¨Šæ˜ å°„ï¼ˆçœŸå¯¦çš„ HTML class/idï¼‰

âš ï¸ **é€™æ˜¯æœ€é‡è¦çš„è³‡æ–™ï¼** ä»¥ä¸‹æ˜¯çœŸå¯¦çš„ HTML å…ƒç´ è³‡è¨Šï¼Œåœ¨å»ºè­°ä¸­å¿…é ˆä½¿ç”¨é€™äº›çœŸå¯¦çš„ class å’Œ idï¼š

```json
{json.dumps(data.get('element_info_map', {}), ensure_ascii=False, indent=2)}
```

 **å¦‚ä½•ä½¿ç”¨**ï¼š
 1. æ‰¾åˆ°å°æ‡‰çš„ target IDï¼ˆä¾‹å¦‚ "item5"ï¼‰
 2. ä½¿ç”¨å…¶ä¸­çš„ `class` æ¬„ä½ä½œç‚º CSS é¸æ“‡å™¨
 3. **é‡è¦ï¼šç°¡åŒ–é¸æ“‡å™¨**
@@
 5. **çµ•å°ä¸è¦çŒœæ¸¬æˆ–ç·¨é€  class åç¨±**
+
+âš ï¸ **è¼¸å‡ºè¦å‰‡è£œå……ï¼ˆéå¸¸é‡è¦ï¼‰**ï¼š
+åœ¨æ‰€æœ‰æ”¹å–„å»ºè­°ä¸­ï¼Œè«‹åŒæ™‚è¼¸å‡ºï¼š
+- ä¸€å€‹ã€Œå»ºè­°ä½¿ç”¨çš„ç°¡åŒ– CSS selectorã€ï¼ˆçµ¦å·¥ç¨‹å¸«å¯¦ä½œï¼‰
+- ä»¥åŠå°æ‡‰çš„ã€Œå®Œæ•´åŸå§‹ DOM è³‡è¨Šï¼ˆraw class / tagï¼‰ã€ä½œç‚ºè­‰æ“š
+
+è«‹æ³¨æ„ï¼š
+- ç°¡åŒ– selector èˆ‡ raw class å¿…é ˆä¾†è‡ªåŒä¸€å€‹ target_id
+- raw class å¿…é ˆå®Œæ•´é€å­—è¼¸å‡ºï¼Œä¸å¯çœç•¥ã€ä¸å¯é‡çµ„

---

## ğŸ’­ ä½¿ç”¨è€…å…§å¿ƒæƒ³æ³•ï¼ˆæœ€é‡è¦çš„æ•¸æ“šï¼ï¼‰

å…± {data['total_thoughts']} æ¢æƒ³æ³•ï¼š

```json
{json.dumps(data['chinese_thoughts'], ensure_ascii=False, indent=2)}
```

---

# åˆ†æè¦æ±‚

## æ­¥é©Ÿ 1ï¼šæ·±åº¦åˆ†æä½¿ç”¨è€…è¡Œç‚º

é€æ¢é–±è®€æ‰€æœ‰å…§å¿ƒæƒ³æ³•ï¼Œæ‰¾å‡ºï¼š
1. é‡è¤‡å‡ºç¾çš„ç–‘æ…®æˆ–å›°æƒ‘
2. æœªè¢«æ»¿è¶³çš„éœ€æ±‚
3. æ”¾æ£„çš„åŸå› ï¼ˆå¦‚æœä»»å‹™æœªå®Œæˆï¼‰
4. å¿ƒç†è®ŠåŒ–æ¼”é€²

## æ­¥é©Ÿ 2ï¼šè­˜åˆ¥ UX å•é¡Œ

æ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ï¼š
1. å…·é«”çš„å•é¡Œæè¿°
2. å¼•ç”¨è‡³å°‘ 3-5 æ¢ä½¿ç”¨è€…æƒ³æ³•ï¼ˆå®Œæ•´åŸæ–‡ï¼‰
3. å¾ã€Œç²¾ç¢ºçµ±è¨ˆæ•¸æ“šã€ä¸­å¼•ç”¨é»æ“Šæ¬¡æ•¸
4. åš´é‡ç¨‹åº¦åˆ¤æ–·ï¼ˆCRITICAL/HIGH/MEDIUM/LOWï¼‰

## æ­¥é©Ÿ 3ï¼šç”¢ç”Ÿæ”¹å–„å»ºè­°

æ¯å€‹å»ºè­°å¿…é ˆåŒ…å«ï¼š
1. å„ªå…ˆç´šï¼ˆP0/P1/P2ï¼‰
2. å…·é«”è¡Œå‹•ï¼ˆå¿…é ˆæ¨™ç¤ºã€é é¢ã€‘å’Œã€å…ƒç´ ä½ç½®ã€‘ï¼‰
3. CSS è®Šæ›´ï¼ˆå¿…é ˆä½¿ç”¨å…ƒç´ è³‡è¨Šæ˜ å°„ä¸­çš„çœŸå¯¦ class/idï¼‰

**CSS å»ºè­°æ ¼å¼ç¯„ä¾‹**ï¼š
```
ã€ç”¢å“é ã€‘çš„ button.add-to-cart (target='item12', class='add-to-cart')ï¼š
- å°‡ font-size å¾ 14px æ”¹ç‚º 16px
- å°‡ padding å¾ 8px æ”¹ç‚º 12px
- å°‡ background-color æ”¹ç‚ºæ›´é«˜å°æ¯”çš„é¡è‰²
```

---

# è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰

```json
{{
  "åŸ·è¡Œæ‘˜è¦": {{
    "ä»»å‹™å®Œæˆ": true/false,
    "å®ŒæˆåŸå› ": "ç°¡çŸ­èªªæ˜",
    "é—œéµæ´å¯Ÿ": "æœ€é‡è¦çš„ç™¼ç¾"
  }},
  "UXå•é¡Œ": [
    {{
      "æ¨™é¡Œ": "å•é¡Œæ¨™é¡Œ",
      "åš´é‡ç¨‹åº¦": "CRITICAL/HIGH/MEDIUM/LOW",
      "é¡åˆ¥": "å°èˆª/è³‡è¨Šå‘ˆç¾/äº’å‹•å›é¥‹/ä¿¡ä»»å»ºç«‹/å…¶ä»–",
      "æè¿°": "å…·é«”çš„å•é¡Œæè¿°",
      "ä½¿ç”¨è€…æƒ³æ³•": [
        "ä½¿ç”¨è€…æƒ³æ³•åŸæ–‡1",
        "ä½¿ç”¨è€…æƒ³æ³•åŸæ–‡2",
        "ä½¿ç”¨è€…æƒ³æ³•åŸæ–‡3"
      ],
      "å½±éŸ¿": "æ­¤å•é¡Œå°ä½¿ç”¨è€…é«”é©—çš„å½±éŸ¿",
      "è­‰æ“š": "å¾ç²¾ç¢ºçµ±è¨ˆå¼•ç”¨ï¼šé»æ“Š target='itemXX' å…± X æ¬¡"
    }}
  ],
  "æ”¹å–„å»ºè­°": [
    {{
      "å„ªå…ˆç´š": "P0/P1/P2",
      "æ¨™é¡Œ": "å»ºè­°æ¨™é¡Œ",
      "é¡åˆ¥": "å°æ‡‰å•é¡Œçš„é¡åˆ¥",
      "ç†ç”±": "ç‚ºä»€éº¼è¦é€™æ¨£åš",
      "å…·é«”è¡Œå‹•": [
        "ã€é é¢åç¨±ã€‘çš„ã€å…ƒç´ ä½ç½®ã€‘ï¼ˆclass='.real-class', target='itemXX'ï¼‰ï¼šå…·é«”è®Šæ›´"
      ],
      "CSSè®Šæ›´": [
        {{
            "target_id": "itemXX",
        +   "raw_dom": {
        +     "tag": "button",
        +     "class": "sale-page-btn core-btn add-to-cart-btn custom-btn cms-secondBtnBgColor cms-secondBtnTextColor cms-secondBtnBorderColor",
        +     "id": ""
        +   },
                "é¸æ“‡å™¨": ".add-to-cart-btn",
                "é¸æ“‡å™¨èªªæ˜": "å¾ class ä¸­é¸æ“‡æœ€å…·èªæ„çš„ä¸€å€‹ï¼ˆä¸è¦å…¨éƒ¨åˆ—å‡ºï¼‰",
                "å±¬æ€§": "font-size",
                "ç›®å‰å€¼": "14px",
                "å»ºè­°å€¼": "16px",
                "åŸå› ": "æå‡å¯è®€æ€§"
        }}
      ],
      "é æœŸæ•ˆæœ": "é‡åŒ–çš„é æœŸæ•ˆæœ"
    }}
  ]
}}
```

**æª¢æŸ¥æ¸…å–®**ï¼š
âœ… æ‰€æœ‰é»æ“Šæ¬¡æ•¸ä¾†è‡ªç²¾ç¢ºçµ±è¨ˆæ•¸æ“šï¼Ÿ
âœ… CSS é¸æ“‡å™¨ä¾†è‡ªå…ƒç´ è³‡è¨Šæ˜ å°„çš„çœŸå¯¦ classï¼Ÿ
âœ… æ¯å€‹å•é¡Œéƒ½å¼•ç”¨äº† 3+ æ¢ä½¿ç”¨è€…æƒ³æ³•ï¼Ÿ
âœ… å»ºè­°å¤ å…·é«”ï¼ˆæœ‰å¯¦éš›çš„ CSS å±¬æ€§å’Œæ•¸å€¼ï¼‰ï¼Ÿ

è«‹ç›´æ¥è¼¸å‡º JSONã€‚
"""
        return prompt

    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æ LLM çš„ JSON å›æ‡‰"""
        try:
            return json.loads(response_text.strip())
        except json.JSONDecodeError as e:
            print(f"âŒ è§£æ LLM å›æ‡‰å¤±æ•—: {e}")
            print(f"å›æ‡‰å…§å®¹:\n{response_text[:500]}...")
            return {
                "error": "Failed to parse LLM response",
                "raw_response": response_text
            }


# ==================== Report Generator ====================

def generate_markdown_report(analysis: Dict[str, Any], persona: PersonaProfile, run_dir: Path) -> str:
    """æ ¹æ“š LLM åˆ†æçµæœç”¢ç”Ÿ Markdown å ±å‘Š"""

    test_metadata = analysis.get('test_metadata', {})

    report = f"""# UX æ·±åº¦åˆ†æå ±å‘Š

## ğŸ“‹ æ¸¬è©¦è³‡è¨Š

| é …ç›® | å…§å®¹ |
|------|------|
| **æ¸¬è©¦ç›®éŒ„** | {test_metadata.get('run_directory', run_dir.name)} |
| **Persona æª”æ¡ˆ** | {test_metadata.get('persona_file', 'N/A')} |
| **Persona å§“å** | {test_metadata.get('persona_name', persona.name)} |
| **Persona é¡å‹** | {test_metadata.get('persona_type', persona.persona_type)} |
| **åˆ†ææ™‚é–“** | {test_metadata.get('analysis_timestamp', datetime.now().isoformat())} |
| **ç¸½æ“ä½œæ¬¡æ•¸** | {test_metadata.get('total_actions', 'N/A')} |
| **ç¸½æ€è€ƒæ¬¡æ•¸** | {test_metadata.get('total_thoughts', 'N/A')} |

---

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

### ä»»å‹™å®Œæˆç‹€æ…‹
- **å®Œæˆ**: {'âœ… æ˜¯' if analysis.get('åŸ·è¡Œæ‘˜è¦', {}).get('ä»»å‹™å®Œæˆ') else 'âŒ å¦'}
- **åŸå› **: {analysis.get('åŸ·è¡Œæ‘˜è¦', {}).get('å®ŒæˆåŸå› ', 'N/A')}

### é—œéµæ´å¯Ÿ
> {analysis.get('åŸ·è¡Œæ‘˜è¦', {}).get('é—œéµæ´å¯Ÿ', 'N/A')}

---

## ğŸš¨ ç™¼ç¾çš„ UX å•é¡Œ

"""

    for i, issue in enumerate(analysis.get('UXå•é¡Œ', []), 1):
        severity_emoji = {
            'CRITICAL': 'ğŸ”´',
            'HIGH': 'ğŸŸ ',
            'MEDIUM': 'ğŸŸ¡',
            'LOW': 'ğŸŸ¢'
        }.get(issue.get('åš´é‡ç¨‹åº¦', 'MEDIUM'), 'âšª')

        report += f"""
### {severity_emoji} å•é¡Œ {i}: {issue.get('æ¨™é¡Œ', 'Unknown')}

- **åš´é‡ç¨‹åº¦**: {issue.get('åš´é‡ç¨‹åº¦', 'N/A')}
- **é¡åˆ¥**: {issue.get('é¡åˆ¥', 'N/A')}

**æè¿°**:
{issue.get('æè¿°', 'N/A')}

**ä½¿ç”¨è€…å…§å¿ƒæƒ³æ³•**:
"""
        for thought in issue.get('ä½¿ç”¨è€…æƒ³æ³•', []):
            report += f'> "{thought}"\n\n'

        report += f"""
**å½±éŸ¿**:
{issue.get('å½±éŸ¿', 'N/A')}

**è­‰æ“š**:
{issue.get('è­‰æ“š', 'N/A')}

---
"""

    report += """
## ğŸ’¡ æ”¹å–„å»ºè­°

"""

    priority_order = {'P0': 1, 'P1': 2, 'P2': 3}
    recommendations = sorted(
        analysis.get('æ”¹å–„å»ºè­°', []),
        key=lambda x: priority_order.get(x.get('å„ªå…ˆç´š', 'P2'), 999)
    )

    for i, rec in enumerate(recommendations, 1):
        priority_emoji = {
            'P0': 'ğŸ”¥',
            'P1': 'âš¡',
            'P2': 'ğŸ“…'
        }.get(rec.get('å„ªå…ˆç´š', 'P2'), 'ğŸ“Œ')

        report += f"""
### {priority_emoji} å»ºè­° {i}: {rec.get('æ¨™é¡Œ', 'Unknown')}

- **å„ªå…ˆç´š**: {rec.get('å„ªå…ˆç´š', 'N/A')}
- **é¡åˆ¥**: {rec.get('é¡åˆ¥', 'N/A')}

**ç†ç”±**:
{rec.get('ç†ç”±', 'N/A')}

**å…·é«”è¡Œå‹•**:
"""
        for action in rec.get('å…·é«”è¡Œå‹•', []):
            report += f"- {action}\n"

        # CSS è®Šæ›´æ¸…å–®
        if rec.get('CSSè®Šæ›´'):
            report += "\n**CSS è®Šæ›´æ˜ç´°**:\n\n"
            for idx, css_change in enumerate(rec['CSSè®Šæ›´'], 1):
                target_id = css_change.get('target_id', 'N/A')
                selector = css_change.get('é¸æ“‡å™¨', 'N/A')
                raw_dom = css_change.get('raw_dom', {}) #é¡¯ç¤º raw DOM
                raw_class = raw_dom.get('class', 'N/A')
                raw_tag = raw_dom.get('tag', 'N/A')
                raw_id = raw_dom.get('id', '')
                property_name = css_change.get('å±¬æ€§', 'N/A')
                current = css_change.get('ç›®å‰å€¼', 'N/A')
                recommended = css_change.get('å»ºè­°å€¼', 'N/A')
                reason = css_change.get('åŸå› ', 'N/A')
                
                report += f"""
<details>
<summary><strong>è®Šæ›´ {idx}</strong>: <code>{property_name}</code> ({target_id})</summary>

- **Target ID**: `{target_id}`
- **CSS é¸æ“‡å™¨**: `{selector}`
- **CSS é¸æ“‡å™¨**: `{selector}`
+ **CSS é¸æ“‡å™¨ï¼ˆå»ºè­°ï¼‰**: `{selector}`
+ **åŸå§‹ DOM tag**: `{raw_tag}`
+ **åŸå§‹ DOM class**: `{raw_class}`
+ **åŸå§‹ DOM id**: `{raw_id}`
- **å±¬æ€§**: `{property_name}`
- **ç›®å‰å€¼**: {current}
- **å»ºè­°å€¼**: {recommended}
- **åŸå› **: {reason}

**ä½¿ç”¨æ–¹å¼**:
```css
{selector} {{
  {property_name}: {recommended};
}}
```

</details>

"""

        report += f"""
**é æœŸæ•ˆæœ**:
{rec.get('é æœŸæ•ˆæœ', 'N/A')}

---
"""

    report += f"""
---

**å ±å‘Šç”¢ç”Ÿæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†æå·¥å…·**: UX Analyzer LLM Deep Analysis v1.0
"""

    return report


# ==================== Main ====================

def main():
    parser = argparse.ArgumentParser(description='UX åˆ†æå™¨ - ç´” LLM æ·±åº¦åˆ†æ')
    parser.add_argument('--run-dir', required=True, help='æ¸¬è©¦çµæœç›®éŒ„')
    parser.add_argument('--persona', required=True, help='Persona æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--api-key', help='OpenAI API keyï¼ˆæˆ–åœ¨ .env ä¸­è¨­å®šï¼‰')
    parser.add_argument('--output', help='è¼¸å‡ºå ±å‘Šè·¯å¾‘')

    args = parser.parse_args()

    run_dir = Path(args.run_dir)
    memory_trace_file = run_dir / 'memory_trace.json'
    action_trace_file = run_dir / 'action_trace.json'
    persona_file = Path(args.persona)

    # æª¢æŸ¥æª”æ¡ˆ
    if not memory_trace_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° memory_trace.json: {memory_trace_file}")
        return
    if not action_trace_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° action_trace.json: {action_trace_file}")
        return
    if not persona_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° persona æª”æ¡ˆ: {persona_file}")
        return

    print(f"ğŸ“Š é–‹å§‹ LLM æ·±åº¦åˆ†æ...")
    print(f"   æ¸¬è©¦ç›®éŒ„: {run_dir}")
    print(f"   Persona: {persona_file}")

    # è®€å–æ•¸æ“š
    with open(memory_trace_file, 'r', encoding='utf-8') as f:
        memory_trace = json.load(f)

    with open(action_trace_file, 'r', encoding='utf-8') as f:
        action_trace = json.load(f)

    # è§£æ persona
    persona = PersonaProfile(str(persona_file))
    print(f"\nğŸ‘¤ Persona: {persona.name} ({persona.persona_type})")

    # åˆå§‹åŒ– LLM åˆ†æå™¨
    try:
        analyzer = LLMUXAnalyzer(api_key=args.api_key)
    except ValueError as e:
        print(f"\nâŒ {e}")
        return

    # åŸ·è¡Œåˆ†æ
    try:
        analysis_result = analyzer.analyze_test_results(
            memory_trace=memory_trace,
            action_trace=action_trace,
            persona=persona,
            persona_file=persona_file.name,
            run_dir=run_dir.name
        )
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return

    # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
    if 'error' in analysis_result:
        print(f"\nâŒ LLM åˆ†æå‡ºéŒ¯")
        print(f"éŒ¯èª¤: {analysis_result.get('error')}")
        print(f"\nåŸå§‹å›æ‡‰:\n{analysis_result.get('raw_response', '')[:1000]}...")
        return

    # å„²å­˜ JSON çµæœ
    json_output = args.output or str(run_dir / 'ux_analysis_llm.json')
    with open(json_output, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON åˆ†æçµæœå·²å„²å­˜: {json_output}")

    # ç”¢ç”Ÿ Markdown å ±å‘Š
    md_output = str(run_dir / 'ux_analysis_llm.md')
    report = generate_markdown_report(analysis_result, persona, run_dir)
    Path(md_output).write_text(report, encoding='utf-8')
    print(f"âœ… Markdown å ±å‘Šå·²ç”¢ç”Ÿ: {md_output}")

    # é¡¯ç¤ºæ‘˜è¦
    print("\n" + "="*60)
    print("LLM åˆ†ææ‘˜è¦")
    print("="*60)

    summary = analysis_result.get('åŸ·è¡Œæ‘˜è¦', {})
    print(f"\nä»»å‹™å®Œæˆ: {'âœ… æ˜¯' if summary.get('ä»»å‹™å®Œæˆ') else 'âŒ å¦'}")
    print(f"åŸå› : {summary.get('å®ŒæˆåŸå› ', 'N/A')}")
    print(f"\né—œéµæ´å¯Ÿ: {summary.get('é—œéµæ´å¯Ÿ', 'N/A')}")

    print(f"\nç™¼ç¾ {len(analysis_result.get('UXå•é¡Œ', []))} å€‹ UX å•é¡Œ")
    print(f"ç”¢ç”Ÿ {len(analysis_result.get('æ”¹å–„å»ºè­°', []))} é …æ”¹å–„å»ºè­°")

    print(f"\nğŸ“„ å®Œæ•´å ±å‘Šè«‹è¦‹: {md_output}")


if __name__ == '__main__':
    main()